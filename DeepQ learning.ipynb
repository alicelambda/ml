{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Deep Q With Expercience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-ram-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"recording\",\n",
    "                           video_callable= lambda x:x % 20 == 0,\n",
    "                          force=True)\n",
    "INPUT_SHAPE = env.observation_space.shape\n",
    "print(INPUT_SHAPE)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "print(NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createRamModel(): \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu',input_shape=INPUT_SHAPE))\n",
    "    # output is the same size as number of outputs\n",
    "    model.add(layers.Dense(70,activation='relu'))\n",
    "    model.add(layers.Dense(60,activation='relu'))\n",
    "    model.add(layers.Dense(40,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(NUM_ACTIONS,activation='linear'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])  # mean absolute error\n",
    "    return model\n",
    "\n",
    "\n",
    "training_model = createRamModel()\n",
    "target_model = createRamModel()\n",
    "target_model.set_weights(training_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1122.5303 - mae: 13.8451\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13808.8145 - mae: 48.4074\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5389.7290 - mae: 29.3097\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19907.9961 - mae: 61.9176\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80902.2031 - mae: 127.8016\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102908.0469 - mae: 139.1658\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18065.1973 - mae: 56.3019\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 191712.4062 - mae: 183.0362\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231233.7812 - mae: 189.1891\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 361200.9375 - mae: 237.9421\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 824177.0000 - mae: 377.0165\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1324237.6250 - mae: 451.3099\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4628529.5000 - mae: 867.6923\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7152501.0000 - mae: 1074.8313\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4800765.0000 - mae: 847.8856\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24498504.0000 - mae: 2114.9077\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48501916.0000 - mae: 2741.4319\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32013066.0000 - mae: 2421.5469\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 24643062.0000 - mae: 2051.2642\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 52408444.0000 - mae: 3024.6270\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 68055344.0000 - mae: 3330.4163\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 52032192.0000 - mae: 2882.9719\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 78290392.0000 - mae: 3710.0457\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 90432256.0000 - mae: 3800.8604\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 114975168.0000 - mae: 4508.3794\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 240451680.0000 - mae: 6325.1880\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 143351360.0000 - mae: 4741.9546\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 311817504.0000 - mae: 7456.5000\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 443763392.0000 - mae: 8776.6377\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 490881344.0000 - mae: 9015.9355\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 846583808.0000 - mae: 12481.1738\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 495225184.0000 - mae: 8683.1602\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 618820160.0000 - mae: 9609.0410\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 758228224.0000 - mae: 11489.1064\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 451471008.0000 - mae: 8645.8135\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 546037120.0000 - mae: 9357.4775\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 560990144.0000 - mae: 9740.1436\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 574383616.0000 - mae: 9909.9443\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 507413824.0000 - mae: 9264.3506\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 462298464.0000 - mae: 8788.9668\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 179248624.0000 - mae: 5137.8809\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 891560768.0000 - mae: 12480.9424\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 540183616.0000 - mae: 10354.2109\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 272233888.0000 - mae: 6595.9062\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 248448144.0000 - mae: 6301.1904\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 621573504.0000 - mae: 10034.8252\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 269114752.0000 - mae: 7286.8779\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 388717248.0000 - mae: 8532.1729\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 216245184.0000 - mae: 6020.5371\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 203591264.0000 - mae: 5668.9678\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 234111072.0000 - mae: 5793.5537\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 190077952.0000 - mae: 5527.7275\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 192034880.0000 - mae: 5775.8057\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 119683392.0000 - mae: 4529.6914\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 106139248.0000 - mae: 3922.7100\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 109311056.0000 - mae: 4255.9033\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 103572800.0000 - mae: 4041.3608\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 102167392.0000 - mae: 4220.1606\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 86954016.0000 - mae: 3726.0488\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49083680.0000 - mae: 2475.5039\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48062612.0000 - mae: 2869.9834\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 47451272.0000 - mae: 2779.2559\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 54475968.0000 - mae: 2831.7988\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 40604612.0000 - mae: 2528.3623\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35957896.0000 - mae: 2398.2070\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33695976.0000 - mae: 2391.7729\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23722340.0000 - mae: 1921.5374\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37386040.0000 - mae: 2500.8096\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19605722.0000 - mae: 1829.0310\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 18965878.0000 - mae: 1654.9036\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15461298.0000 - mae: 1551.9749\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19820452.0000 - mae: 1730.2246\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13800386.0000 - mae: 1436.1816\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15321168.0000 - mae: 1619.2917\n",
      "training 0.96059601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 15884498.0000 - mae: 1654.4663\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14154104.0000 - mae: 1488.6926\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14510280.0000 - mae: 1587.7424\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11839716.0000 - mae: 1359.9263\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 14427590.0000 - mae: 1598.5618\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15715282.0000 - mae: 1613.5566\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8835090.0000 - mae: 1242.2122\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8098435.5000 - mae: 1121.2281\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9449458.0000 - mae: 1250.4292\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5734531.5000 - mae: 920.4757\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7172077.0000 - mae: 1074.1196\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7281297.0000 - mae: 1046.0355\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6786897.0000 - mae: 1014.8645\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6337762.0000 - mae: 1016.2382\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6110712.5000 - mae: 975.7609\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6970728.0000 - mae: 996.4791\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5975498.0000 - mae: 952.4794\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4276047.0000 - mae: 795.6038\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3657551.2500 - mae: 699.1316\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3047462.0000 - mae: 669.0269\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3919736.5000 - mae: 764.5427\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5653211.0000 - mae: 933.4773\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4586368.5000 - mae: 907.2385\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4187161.0000 - mae: 831.5658\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4934625.0000 - mae: 879.2035\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5700622.0000 - mae: 936.7483\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4982006.0000 - mae: 932.3037\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3436183.0000 - mae: 749.3851\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4606784.0000 - mae: 851.0747\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5500524.0000 - mae: 871.8025\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3819547.5000 - mae: 760.0890\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3939260.0000 - mae: 784.3270\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3928599.5000 - mae: 800.9590\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2973532.0000 - mae: 701.3431\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3889494.5000 - mae: 703.9830\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3539492.0000 - mae: 747.8452\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3493358.0000 - mae: 769.3715\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2403455.5000 - mae: 644.1958\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4888631.0000 - mae: 884.8927\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2619075.5000 - mae: 651.3417\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3925581.0000 - mae: 811.4423\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4025604.5000 - mae: 819.0427\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3650557.7500 - mae: 759.5501\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2609908.5000 - mae: 653.9095\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3164135.0000 - mae: 761.1125\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2417034.0000 - mae: 639.0779\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2370751.7500 - mae: 635.8131\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2368315.0000 - mae: 618.2535\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3121108.5000 - mae: 680.6859\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3147140.7500 - mae: 683.0272\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2973234.0000 - mae: 661.3319\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2078953.0000 - mae: 525.6970\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2560143.5000 - mae: 618.9547\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2150828.2500 - mae: 590.7759\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2366838.7500 - mae: 583.0292\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2368155.0000 - mae: 624.0380\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1855984.0000 - mae: 544.3765\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3089860.2500 - mae: 686.9874\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2564283.7500 - mae: 640.3016\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3305885.5000 - mae: 755.7343\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1510528.2500 - mae: 484.2181\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1238048.2500 - mae: 447.0245\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1890226.5000 - mae: 526.1069\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1531681.0000 - mae: 489.6116\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2952949.5000 - mae: 662.0518\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2908690.5000 - mae: 624.9428\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3156811.0000 - mae: 701.1618\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1705563.0000 - mae: 502.1185\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1296667.2500 - mae: 474.1605\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3068080.0000 - mae: 665.8369\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2517463.0000 - mae: 650.5692\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2797474.0000 - mae: 660.8545\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2569189.0000 - mae: 652.2728\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1792983.2500 - mae: 541.9164\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1660570.2500 - mae: 511.3623\n",
      "training 0.96059601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 1666826.2500 - mae: 490.7576\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1691878.1250 - mae: 528.4092\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2044864.6250 - mae: 531.9203\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2285157.0000 - mae: 613.2330\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2678158.0000 - mae: 661.3494\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1194906.2500 - mae: 436.7012\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1217291.0000 - mae: 448.3392\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2158071.5000 - mae: 559.0220\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1633483.5000 - mae: 475.8654\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1506453.6250 - mae: 451.6688\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1672292.1250 - mae: 488.5190\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1657660.0000 - mae: 517.3376\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2045750.5000 - mae: 528.1938\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2173045.0000 - mae: 592.9603\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1247953.7500 - mae: 404.1305\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1735866.0000 - mae: 520.5242\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 940203.2500 - mae: 368.0136\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1783925.7500 - mae: 532.9406\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1194682.7500 - mae: 445.0610\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1458114.7500 - mae: 465.7980\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1096268.2500 - mae: 438.1053\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1731530.2500 - mae: 523.0219\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2536509.0000 - mae: 604.2913\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1021383.8750 - mae: 419.4803\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1101397.5000 - mae: 413.0972\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1127351.3750 - mae: 427.1492\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1485635.0000 - mae: 503.6605\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1182478.7500 - mae: 425.5160\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1154453.2500 - mae: 418.8293\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1303070.5000 - mae: 454.2775\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1930692.5000 - mae: 539.9887\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1454071.7500 - mae: 475.2791\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1497668.5000 - mae: 468.9080\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1487532.3750 - mae: 486.1584\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1173837.8750 - mae: 448.9457\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1848082.5000 - mae: 541.0034\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1763298.0000 - mae: 535.1655\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1492402.0000 - mae: 521.8325\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 910494.4375 - mae: 388.2543\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1941396.1250 - mae: 566.4541\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1198393.5000 - mae: 403.3372\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1267759.6250 - mae: 455.3775\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 974796.5000 - mae: 398.3172\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1339354.2500 - mae: 480.1552\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1397870.2500 - mae: 433.9595\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1346471.3750 - mae: 445.1451\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1611077.2500 - mae: 504.5526\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1296391.6250 - mae: 431.7112\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 869714.6875 - mae: 368.3151\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1124989.2500 - mae: 391.8521\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2014661.3750 - mae: 554.5978\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1105819.8750 - mae: 420.9807\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1097639.5000 - mae: 381.7711\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1140391.7500 - mae: 413.1475\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1114969.7500 - mae: 397.7676\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1041956.0000 - mae: 404.4622\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1006290.7500 - mae: 379.8174\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1779581.7500 - mae: 514.9869\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 877963.0000 - mae: 351.8127\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1119068.6250 - mae: 440.2064\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1035094.5000 - mae: 419.9934\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 872285.7500 - mae: 382.7352\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 967827.8125 - mae: 402.3422\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1018582.8750 - mae: 400.2306\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1002400.5625 - mae: 386.5339\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 965763.0000 - mae: 350.9613\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 937994.0000 - mae: 393.0027\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 962036.6250 - mae: 382.1425\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1186374.5000 - mae: 399.3662\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1043477.4375 - mae: 403.9039\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 895838.5000 - mae: 350.3304\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1288136.5000 - mae: 440.2780\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 831192.5000 - mae: 356.3582\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 782611.8125 - mae: 351.2390\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 924719.3750 - mae: 374.9145\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 465511.5625 - mae: 281.9239\n",
      "training 0.96059601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 1001768.0625 - mae: 388.3987\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1310685.0000 - mae: 434.0882\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1042823.1250 - mae: 393.6036\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 975251.8750 - mae: 384.9691\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 800946.0000 - mae: 359.7991\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 829208.6250 - mae: 366.0644\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 863587.3750 - mae: 389.6469\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 808650.8750 - mae: 354.8625\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 836446.0000 - mae: 344.2478\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1342409.5000 - mae: 448.7752\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1014467.7500 - mae: 399.3190\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 747101.8750 - mae: 331.5836\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 679998.6250 - mae: 327.3173\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1018893.3750 - mae: 369.4491\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 975079.1250 - mae: 394.5700\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 961705.5000 - mae: 372.9495\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1150753.5000 - mae: 420.4717\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 706049.9375 - mae: 332.3711\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 928761.0000 - mae: 370.0895\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 994963.3750 - mae: 417.1570\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 771413.8750 - mae: 345.4154\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1672102.7500 - mae: 509.2726\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 678189.3750 - mae: 336.8514\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 663262.8125 - mae: 311.5903\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1080988.7500 - mae: 424.4377\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 542833.7500 - mae: 288.5245\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 642849.3750 - mae: 319.6683\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 808920.6250 - mae: 364.6847\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1243246.0000 - mae: 447.7243\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 839213.5000 - mae: 350.0145\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 919206.8750 - mae: 364.1629\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 986661.0000 - mae: 386.3436\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 726288.0000 - mae: 358.4955\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 692319.3125 - mae: 326.0303\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 769283.0000 - mae: 337.4467\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 497799.0625 - mae: 304.9236\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 663749.0625 - mae: 307.6551\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 866882.6250 - mae: 351.7148\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 723592.8750 - mae: 333.1967\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 726810.2500 - mae: 336.4711\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 637815.8125 - mae: 308.0271\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 898292.6250 - mae: 375.3551\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 595040.1875 - mae: 295.8328\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 456261.9062 - mae: 259.5542\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 827510.7500 - mae: 351.7491\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 638443.0000 - mae: 315.7340\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 828318.7500 - mae: 363.5964\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 544932.1875 - mae: 280.0902\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 602624.8750 - mae: 307.3831\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1155461.0000 - mae: 448.2602\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 698798.0000 - mae: 332.4500\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 949536.5625 - mae: 377.9111\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 823904.3750 - mae: 334.6901\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 705033.3750 - mae: 323.5492\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 704014.6250 - mae: 336.7760\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 679064.1250 - mae: 319.3751\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 781785.8750 - mae: 339.1413\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 680836.2500 - mae: 323.0457\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 502097.6875 - mae: 272.5068\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 403341.6250 - mae: 265.4064\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 759451.0625 - mae: 347.8021\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 519452.6875 - mae: 292.9405\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 468786.5000 - mae: 277.8162\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 517406.0000 - mae: 297.0972\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 469122.0312 - mae: 274.6382\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 773881.3750 - mae: 337.6352\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 625993.2500 - mae: 307.7937\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 564392.8750 - mae: 303.8709\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 620588.4375 - mae: 328.2964\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 482837.5312 - mae: 276.1315\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 609296.7500 - mae: 294.2935\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 741141.5000 - mae: 337.9187\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 499360.9375 - mae: 274.7326\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1014987.3125 - mae: 391.5558\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 820621.5625 - mae: 345.6133\n",
      "training 0.96059601\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 581394.2500 - mae: 313.7948\n",
      "training 0.96059601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 623563.5625 - mae: 298.7022\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 353653.6875 - mae: 236.6925\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 622159.1250 - mae: 296.7205\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 679932.1250 - mae: 314.4694\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 586302.0000 - mae: 275.6509\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 493199.9375 - mae: 292.1519\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 872951.8750 - mae: 362.6811\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 760436.0000 - mae: 331.0800\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 427144.5000 - mae: 238.3827\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 499060.1875 - mae: 277.3397\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 533209.9375 - mae: 275.5428\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 683835.2500 - mae: 305.2909\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 640718.1875 - mae: 317.0384\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 535635.8750 - mae: 291.6686\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 404866.8125 - mae: 261.0136\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 366832.1875 - mae: 233.4282\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 534072.7500 - mae: 279.7110\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 593024.6875 - mae: 312.9615\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 470605.1250 - mae: 286.4519\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 533756.0000 - mae: 271.8458\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 513761.5625 - mae: 283.6755\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 388910.7500 - mae: 239.0476\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 688239.2500 - mae: 319.3012\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 391934.3438 - mae: 245.3947\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 547893.3750 - mae: 292.6818\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 470462.9375 - mae: 271.8572\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 591446.5000 - mae: 278.3382\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 492429.7812 - mae: 273.7053\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 527976.2500 - mae: 271.0208\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 507251.3438 - mae: 262.2053\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 460518.3750 - mae: 276.9544\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 722111.2500 - mae: 330.9931\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 499261.1875 - mae: 280.6212\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 300479.7188 - mae: 222.4028\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 534295.3750 - mae: 285.1744\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 469244.0625 - mae: 276.1218\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 463442.7500 - mae: 290.5775\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 551738.8750 - mae: 284.2812\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 475080.8125 - mae: 260.7086\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 494318.9062 - mae: 283.8826\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 513351.2500 - mae: 281.3576\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 403783.5000 - mae: 250.5643\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 472127.6875 - mae: 275.0248\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 799478.0625 - mae: 333.5376\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 318406.3438 - mae: 227.7430\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 493795.1875 - mae: 265.9982\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 397300.9062 - mae: 245.9135\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 509218.5625 - mae: 268.6156\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 543039.8750 - mae: 291.4753\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 293949.8438 - mae: 227.7782\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 454025.0625 - mae: 262.2515\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 528332.3125 - mae: 263.9247\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 550299.8750 - mae: 292.7643\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 521056.8125 - mae: 283.7014\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 453839.8750 - mae: 237.0347\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 419085.7188 - mae: 269.4305\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 328418.5625 - mae: 226.4829\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 478775.8750 - mae: 281.6381\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 471260.5000 - mae: 276.7456\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 428884.5000 - mae: 264.2376\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 405635.0000 - mae: 266.5304\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 304792.2188 - mae: 220.0878\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 345697.5625 - mae: 243.2684\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 644315.5000 - mae: 303.6407\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 301352.5000 - mae: 213.0826\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 209734.2656 - mae: 182.6674\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 545403.2500 - mae: 275.0876\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 302078.2500 - mae: 218.7529\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 363604.6250 - mae: 240.1420\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 360277.4688 - mae: 244.3666\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 408244.5625 - mae: 251.0220\n",
      "training 0.9509900498999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 561365.4375 - mae: 305.8240\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 428890.1250 - mae: 240.5572\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 583628.7500 - mae: 303.0588\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 514453.1250 - mae: 279.5262\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 541736.2500 - mae: 309.7074\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 422798.6875 - mae: 247.2008\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 373463.7500 - mae: 255.8931\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 304303.4062 - mae: 228.3387\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 396297.8750 - mae: 233.1827\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 501648.1250 - mae: 281.5686\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 443185.1875 - mae: 273.0275\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249294.2812 - mae: 198.6239\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 426116.1250 - mae: 254.1804\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 473874.3125 - mae: 252.7261\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 494779.4688 - mae: 273.9320\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 349412.6250 - mae: 225.9376\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 449158.1250 - mae: 268.5589\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 397280.9375 - mae: 243.2278\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 744147.2500 - mae: 304.1127\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 396495.9375 - mae: 240.6217\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 296687.2500 - mae: 200.8879\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 421557.8125 - mae: 260.7791\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 397094.5312 - mae: 260.2508\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249553.5156 - mae: 186.3578\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 387668.4375 - mae: 247.5624\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307793.7188 - mae: 196.3059\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 412884.2812 - mae: 266.0761\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 276721.3125 - mae: 214.9055\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 359618.0625 - mae: 220.6697\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 361891.6875 - mae: 214.4947\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 275677.9062 - mae: 195.6066\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 346382.6250 - mae: 219.3858\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 393511.2812 - mae: 239.8177\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 361594.7188 - mae: 232.3216\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 582546.8750 - mae: 299.9068\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 466176.3125 - mae: 252.4084\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 399581.5312 - mae: 230.3356\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 253099.4062 - mae: 194.7937\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 362056.3438 - mae: 241.2502\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 237578.7188 - mae: 194.2241\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 411359.0625 - mae: 262.7418\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 362393.0625 - mae: 246.1960\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 343457.2188 - mae: 233.4711\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 347224.4688 - mae: 220.0099\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 271712.3750 - mae: 215.3692\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 583757.1875 - mae: 295.7354\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 304720.2188 - mae: 219.7753\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 326560.0000 - mae: 217.2378\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 387832.6875 - mae: 228.5789\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 373123.1250 - mae: 243.5353\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 242639.8906 - mae: 195.1328\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 294560.5000 - mae: 215.8221\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 358261.8750 - mae: 223.3658\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 263748.1250 - mae: 201.6982\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 343054.0000 - mae: 238.9473\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 434943.6562 - mae: 248.9082\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 353201.8438 - mae: 239.3948\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 446488.3750 - mae: 243.9173\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 500013.2500 - mae: 269.2346\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 268017.7500 - mae: 188.4911\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 365749.3750 - mae: 216.6526\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 279608.3438 - mae: 203.9488\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 361934.0625 - mae: 239.1425\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 218935.3750 - mae: 184.0862\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 364361.1562 - mae: 230.2001\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 254725.4688 - mae: 201.9491\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 284368.1875 - mae: 212.0968\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 218705.8906 - mae: 199.4033\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 397669.9062 - mae: 251.2482\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 449771.3125 - mae: 252.9541\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 381045.3125 - mae: 241.7108\n",
      "training 0.9509900498999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 396763.0625 - mae: 243.4787\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 312009.7500 - mae: 211.0560\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 315927.5000 - mae: 208.7111\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 405667.3750 - mae: 235.4890\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 318600.7500 - mae: 211.4848\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 303490.2188 - mae: 209.9951\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 331772.2188 - mae: 220.9134\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 442515.8438 - mae: 260.7148\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 276345.9062 - mae: 210.7824\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 213488.6875 - mae: 177.4351\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 332355.1875 - mae: 229.4823\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 248817.0312 - mae: 192.1893\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 239250.0938 - mae: 191.8474\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 370838.5938 - mae: 229.1314\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 352336.4375 - mae: 227.8018\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 318310.0938 - mae: 219.1804\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 449162.3750 - mae: 269.1353\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 338902.8125 - mae: 227.9153\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 500589.2188 - mae: 291.9924\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 491428.6250 - mae: 278.7876\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 251429.4062 - mae: 200.1368\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 336489.5000 - mae: 233.9895\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 310039.5938 - mae: 231.6382\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 297924.8750 - mae: 211.6051\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 418133.7500 - mae: 265.3688\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 236031.5000 - mae: 199.6466\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 213505.4062 - mae: 174.7268\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 385035.6875 - mae: 235.1788\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 210901.3125 - mae: 183.4383\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 231043.2812 - mae: 187.1858\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 412345.3125 - mae: 239.2302\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 461719.9688 - mae: 265.5655\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 197111.0938 - mae: 179.4958\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 354757.0000 - mae: 223.2171\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 404314.1562 - mae: 253.7989\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194133.2812 - mae: 174.5449\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 268087.9375 - mae: 191.4302\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 230102.5781 - mae: 193.1564\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 376481.8438 - mae: 229.1092\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 323340.7188 - mae: 213.8656\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 282598.7500 - mae: 216.6294\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 305881.2500 - mae: 205.5761\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 205820.3438 - mae: 181.7488\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 369538.7500 - mae: 221.9401\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 225092.1250 - mae: 196.4765\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 309066.1250 - mae: 224.2312\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 347581.5000 - mae: 224.8385\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 238463.7812 - mae: 199.6176\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194382.8438 - mae: 177.4234\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 371638.5000 - mae: 236.1064\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 379267.4375 - mae: 238.3001\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 251163.7656 - mae: 195.7946\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 423316.3125 - mae: 252.8994\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 235985.6562 - mae: 189.4005\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 303691.1250 - mae: 216.7616\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 291450.5312 - mae: 211.2842\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 318603.8438 - mae: 217.1990\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 206787.9688 - mae: 177.6383\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 340078.2188 - mae: 230.4347\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 307816.5312 - mae: 203.9491\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 220048.5625 - mae: 186.0733\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 191429.5000 - mae: 169.0153\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 267543.8750 - mae: 185.5343\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 288509.8750 - mae: 204.2312\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 234074.8906 - mae: 191.9558\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 195445.9688 - mae: 167.5322\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 176756.2188 - mae: 163.0496\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 316692.2500 - mae: 231.1872\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 248347.3906 - mae: 197.9773\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 208880.5312 - mae: 181.1481\n",
      "training 0.9509900498999999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 313762.7188 - mae: 206.5353\n",
      "training 0.9509900498999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 376272.7188 - mae: 243.2153\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 263250.5000 - mae: 201.2994\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 178965.1250 - mae: 159.7605\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 372122.9375 - mae: 232.6434\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 326223.5625 - mae: 213.4613\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 268845.1562 - mae: 207.8134\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169152.2500 - mae: 150.2898\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 202406.0469 - mae: 175.2018\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 164543.2656 - mae: 157.7443\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 253911.7031 - mae: 190.6456\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 248716.7500 - mae: 191.5154\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 269675.3125 - mae: 216.4198\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 339762.0000 - mae: 220.7811\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 187361.5000 - mae: 171.1056\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 196214.0469 - mae: 184.0558\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157311.9844 - mae: 150.9598\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 182327.7500 - mae: 166.3081\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 194012.0938 - mae: 175.6375\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 278142.0938 - mae: 217.7565\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 220508.5000 - mae: 180.9742\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 240275.3281 - mae: 199.8507\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 297405.8750 - mae: 216.5927\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222714.7188 - mae: 175.1467\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183112.5312 - mae: 154.4799\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 166720.4375 - mae: 159.7354\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214337.8125 - mae: 166.5973\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 157627.5938 - mae: 162.7927\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 202829.8906 - mae: 185.3184\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 269482.9375 - mae: 206.7237\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 266329.2500 - mae: 190.1521\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 209515.2500 - mae: 180.0037\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 209367.6406 - mae: 168.1507\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 280986.8750 - mae: 198.7897\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 324804.0625 - mae: 210.1621\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 257664.3281 - mae: 190.2206\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 232771.9062 - mae: 204.5788\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 264423.1562 - mae: 182.8024\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 273934.3750 - mae: 200.7089\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 273845.3125 - mae: 206.3376\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 242601.4062 - mae: 185.5271\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 179199.2812 - mae: 165.7770\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 297309.9375 - mae: 224.8592\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 226451.0625 - mae: 179.8281\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 241069.7812 - mae: 191.2262\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 211128.4531 - mae: 175.9827\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 187185.5312 - mae: 173.2808\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 254357.7344 - mae: 183.1789\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 196860.7031 - mae: 177.4838\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 252166.6719 - mae: 203.7497\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198025.0000 - mae: 173.2754\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143495.5312 - mae: 144.9294\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 283274.6562 - mae: 200.2017\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 280483.7188 - mae: 206.8171\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 182405.7656 - mae: 172.6713\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 235888.0938 - mae: 198.0364\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 228202.2812 - mae: 191.9955\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 188598.8906 - mae: 172.0418\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 239527.5938 - mae: 191.5261\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 198308.1250 - mae: 180.8915\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 161732.0625 - mae: 156.9238\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 130352.9844 - mae: 146.7233\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 322524.1875 - mae: 216.5337\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 235403.4531 - mae: 193.3553\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 186651.3281 - mae: 179.9765\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 178212.5312 - mae: 174.6404\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 323755.3750 - mae: 209.5578\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127617.3047 - mae: 140.6056\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 124950.3594 - mae: 133.6430\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 247731.9062 - mae: 194.2383\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 253894.1250 - mae: 185.7697\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 165636.0000 - mae: 159.7140\n",
      "training 0.9414801494009999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 317764.3125 - mae: 219.1116\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 219682.8750 - mae: 170.1318\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 184057.8438 - mae: 176.7726\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101434.1094 - mae: 128.6380\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 190715.3438 - mae: 162.2826\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261257.7969 - mae: 204.1606\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144791.0000 - mae: 150.6549\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 247856.8438 - mae: 198.3100\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 246526.5312 - mae: 196.1366\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249494.5156 - mae: 181.1622\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 267744.4375 - mae: 197.8259\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 238111.2812 - mae: 190.3016\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 137928.2969 - mae: 141.5521\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 133695.3125 - mae: 154.5280\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214085.2031 - mae: 188.4691\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 168595.9688 - mae: 155.5352\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202358.5938 - mae: 172.1883\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 198050.2969 - mae: 158.8039\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 178549.1250 - mae: 170.6749\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145771.7188 - mae: 146.3939\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 192782.3438 - mae: 171.1042\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 266592.3125 - mae: 197.8101\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 221537.1094 - mae: 183.4075\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 283539.8125 - mae: 191.6934\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 213224.1719 - mae: 167.7838\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 206986.5312 - mae: 177.1560\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 330945.6875 - mae: 217.4230\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 217684.8438 - mae: 187.0432\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 217831.6719 - mae: 176.7602\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 214080.4531 - mae: 174.9420\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 202596.6562 - mae: 163.8488\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 106868.7031 - mae: 135.0691\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145472.8438 - mae: 145.7279\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 233937.3438 - mae: 195.7880\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 193969.1250 - mae: 171.4595\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144210.6250 - mae: 146.0672\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 266419.5000 - mae: 198.0149\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137328.4062 - mae: 144.6256\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 262146.5000 - mae: 200.8141\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 126921.3828 - mae: 139.9255\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 227458.9062 - mae: 194.8588\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 176939.8906 - mae: 168.5241\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 141455.4375 - mae: 147.9273\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 222775.4531 - mae: 184.6919\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131577.3125 - mae: 134.2700\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129958.1250 - mae: 143.9596\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 263300.0625 - mae: 203.7529\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 194168.7969 - mae: 155.0896\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 217068.9062 - mae: 171.4494\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 151198.6406 - mae: 135.5365\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 161615.1250 - mae: 149.0754\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110739.8828 - mae: 135.9987\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 188131.9375 - mae: 161.8987\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103473.3672 - mae: 129.0535\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137160.3438 - mae: 133.1899\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 171337.8438 - mae: 154.3089\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 168944.7969 - mae: 147.5913\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148797.6875 - mae: 149.0030\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 68626.2344 - mae: 97.3827\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53832.0781 - mae: 92.8813\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 48066.7109 - mae: 88.5463\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39179.5000 - mae: 79.1290\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47163.2656 - mae: 79.1223\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12978.2500 - mae: 43.5676\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20846.1328 - mae: 57.2257\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18715.2969 - mae: 56.3861\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22515.4180 - mae: 54.2218\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17760.7344 - mae: 50.7351\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21897.4707 - mae: 59.5974\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30785.5000 - mae: 67.7203\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17333.3691 - mae: 54.0014\n",
      "training 0.9414801494009999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 26248.0449 - mae: 56.4652\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28454.5723 - mae: 62.3353\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12728.9990 - mae: 44.6251\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 25445.0820 - mae: 60.2447\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16977.6465 - mae: 52.1848\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17706.9590 - mae: 49.4726\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5583.3701 - mae: 27.2571\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 21581.2109 - mae: 62.1091\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11160.1523 - mae: 39.9592\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13191.0078 - mae: 40.8170\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8907.1357 - mae: 33.6751\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24712.4375 - mae: 64.5764\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9013.0400 - mae: 35.5876\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26090.6914 - mae: 66.6072\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13766.9238 - mae: 48.0174\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25675.8750 - mae: 62.3557\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41837.3203 - mae: 80.1252\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 60709.3086 - mae: 98.0107\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78273.7500 - mae: 100.5062\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 118267.7266 - mae: 141.7527\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 101323.9531 - mae: 134.8120\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 281213.4062 - mae: 227.9359\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 302955.0000 - mae: 228.1579\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 206063.2500 - mae: 177.2833\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 496099.4688 - mae: 287.3794\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1085169.6250 - mae: 420.5181\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 893607.2500 - mae: 369.4108\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 609982.0625 - mae: 313.5740\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 603488.0000 - mae: 310.5419\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 703736.1250 - mae: 362.8517\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1465619.7500 - mae: 478.2266\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 417428.6250 - mae: 271.1652\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 921752.8125 - mae: 357.1451\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1188193.5000 - mae: 468.6745\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1157949.2500 - mae: 405.7089\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1368004.7500 - mae: 453.4309\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1078574.0000 - mae: 408.5606\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1548820.2500 - mae: 467.5492\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1769660.3750 - mae: 531.9314\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2610801.5000 - mae: 654.9287\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1851620.6250 - mae: 529.4253\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2310592.0000 - mae: 612.8238\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3034581.0000 - mae: 699.4467\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1090983.2500 - mae: 398.9118\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2471529.7500 - mae: 611.2625\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2176190.0000 - mae: 595.4801\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3476522.7500 - mae: 706.1899\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3193603.5000 - mae: 712.0557\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2189530.7500 - mae: 553.0535\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2844306.0000 - mae: 653.2027\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2778881.0000 - mae: 656.0721\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1588639.5000 - mae: 510.2203\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2672721.7500 - mae: 672.4938\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2458623.7500 - mae: 574.7891\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3225887.0000 - mae: 676.1337\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3407951.5000 - mae: 738.0893\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2816257.0000 - mae: 622.7048\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3010559.7500 - mae: 706.2578\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2500366.2500 - mae: 609.7244\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2154637.0000 - mae: 603.8190\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8682614.0000 - mae: 1304.8320\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9022951.0000 - mae: 1324.4369\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4146809.2500 - mae: 814.2019\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3608867.5000 - mae: 775.3776\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11710504.0000 - mae: 1409.1932\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4415276.0000 - mae: 808.9171\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7872925.0000 - mae: 1130.4375\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6991335.5000 - mae: 1052.3474\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8711392.0000 - mae: 1217.9374\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7319523.5000 - mae: 1056.6891\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5995540.5000 - mae: 909.1194\n",
      "training 0.9414801494009999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 6193884.0000 - mae: 971.0647\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7699021.0000 - mae: 1097.3430\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7681884.0000 - mae: 1098.9857\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4424582.5000 - mae: 831.8051\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17216616.0000 - mae: 1811.3258\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19440708.0000 - mae: 1801.6876\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8971220.0000 - mae: 1217.8456\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18446542.0000 - mae: 1749.6196\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14924411.0000 - mae: 1517.4128\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7884923.5000 - mae: 1105.4182\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16006869.0000 - mae: 1601.8135\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9605235.0000 - mae: 1270.5679\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11478956.0000 - mae: 1372.4551\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12363331.0000 - mae: 1414.6624\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11959938.0000 - mae: 1462.8840\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14068209.0000 - mae: 1611.1951\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5682952.5000 - mae: 979.6853\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6347533.0000 - mae: 1066.3920\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4654077.0000 - mae: 849.6132\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2053542.5000 - mae: 593.7145\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2964919.0000 - mae: 689.3788\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2241520.5000 - mae: 631.5138\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1339796.3750 - mae: 446.6080\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1141661.6250 - mae: 419.3240\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1209184.8750 - mae: 443.7485\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1001823.2500 - mae: 403.8135\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1212916.0000 - mae: 457.4178\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 905678.6250 - mae: 380.3035\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 713848.9375 - mae: 335.3954\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 928548.2500 - mae: 385.6981\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 566023.5000 - mae: 285.0259\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 542699.0625 - mae: 278.6181\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 289711.2500 - mae: 209.6123\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 356645.1875 - mae: 233.0763\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 327045.3125 - mae: 220.3112\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 436807.6250 - mae: 255.2301\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 405809.1250 - mae: 240.2359\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 225538.1875 - mae: 189.3843\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 264677.5625 - mae: 200.6928\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 298793.3750 - mae: 222.1260\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 328928.5312 - mae: 227.1847\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 261831.2344 - mae: 197.4378\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 242761.5938 - mae: 191.8975\n",
      "training 0.9414801494009999\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 334583.9375 - mae: 216.8049\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 241254.8906 - mae: 199.2157\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145871.5625 - mae: 144.0196\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 138157.8594 - mae: 148.2089\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 211708.5312 - mae: 174.2802\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 161896.0000 - mae: 165.1189\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 177537.7031 - mae: 165.2153\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152449.0312 - mae: 158.8061\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169009.3281 - mae: 165.2672\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135711.3594 - mae: 141.0902\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 232329.1719 - mae: 188.2644\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 209524.0000 - mae: 178.4550\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 207534.7812 - mae: 176.4667\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - ETA: 0s - loss: 128969.1172 - mae: 140.0545WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.181378). Check your callbacks.\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 128969.1172 - mae: 140.0545\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 93661.2500 - mae: 111.1543\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 195591.0312 - mae: 160.1057\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 137460.6719 - mae: 147.6786\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 144284.6562 - mae: 155.6955\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83700.7656 - mae: 113.8001\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 146060.2812 - mae: 137.9924\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 86673.2188 - mae: 111.9096\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 77200.2891 - mae: 113.9874\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 173035.7500 - mae: 162.9139\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103765.8125 - mae: 126.9401\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 183641.3750 - mae: 167.2850\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 100896.7500 - mae: 115.5177\n",
      "training 0.9320653479069899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 92266.4141 - mae: 115.7088\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130801.8125 - mae: 140.5986\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 196972.7188 - mae: 168.5461\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 147038.0156 - mae: 147.0097\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82947.5000 - mae: 120.2826\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 97716.4922 - mae: 130.4028\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 186359.9688 - mae: 175.1950\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156398.9062 - mae: 158.8076\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143303.0000 - mae: 156.6776\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123148.4219 - mae: 124.9736\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 116120.8516 - mae: 119.9467\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 92769.2734 - mae: 120.1335\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 101515.7969 - mae: 121.5129\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123562.3125 - mae: 131.6421\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156199.8438 - mae: 162.4161\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136794.1562 - mae: 140.4454\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 105034.8438 - mae: 118.0992\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 130973.2656 - mae: 137.5485\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73809.9688 - mae: 96.1828\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80472.7656 - mae: 109.6106\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80356.3594 - mae: 111.7386\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 138013.4688 - mae: 148.4801\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 170045.9219 - mae: 155.5342\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61982.8477 - mae: 99.2719\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118720.4688 - mae: 126.5941\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 171711.9375 - mae: 150.5014\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 111675.9531 - mae: 126.2373\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86951.8984 - mae: 118.5199\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45639.2891 - mae: 87.2954\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68609.7344 - mae: 106.5663\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121358.1406 - mae: 132.6528\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102612.7422 - mae: 125.6965\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131901.3438 - mae: 139.1769\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76967.2969 - mae: 109.0760\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62421.8516 - mae: 100.5943\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 112568.2031 - mae: 130.9845\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 82597.8750 - mae: 111.4344\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 102839.1641 - mae: 128.8357\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 120658.7109 - mae: 136.8269\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 110187.0469 - mae: 136.3971\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81098.8906 - mae: 107.5440\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 64411.8086 - mae: 103.7355\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 163665.1875 - mae: 163.7050\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117211.1562 - mae: 129.5852\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88115.2578 - mae: 121.7750\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 61115.6406 - mae: 93.4934\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132882.1562 - mae: 148.7835\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75004.5469 - mae: 102.9738\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 122244.1719 - mae: 140.8971\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 100554.3203 - mae: 124.7143\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 98623.6250 - mae: 117.6481\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 74547.8438 - mae: 110.2181\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95920.0000 - mae: 123.4179\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 98773.6250 - mae: 119.0982\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 113298.4609 - mae: 147.2421\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49588.5156 - mae: 84.5357\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 79852.1406 - mae: 110.9963\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132613.4531 - mae: 138.4783\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127837.7500 - mae: 131.6176\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122274.0625 - mae: 136.1008\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 59745.9609 - mae: 97.5807\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 81844.9531 - mae: 112.9112\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 97545.5391 - mae: 122.7755\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 146950.9062 - mae: 149.2896\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 80085.8281 - mae: 103.8439\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 124927.8750 - mae: 142.1548\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 109880.9531 - mae: 130.3664\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94484.6250 - mae: 121.9898\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74298.4609 - mae: 99.3522\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 55171.2773 - mae: 91.7719\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 71542.3906 - mae: 99.6410\n",
      "training 0.9320653479069899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 71336.5391 - mae: 101.0564\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 74315.5469 - mae: 103.7386\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 69966.0781 - mae: 101.0386\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57866.7617 - mae: 95.3107\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62789.4766 - mae: 97.0089\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62374.6719 - mae: 96.8492\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51266.5547 - mae: 88.1167\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 96501.9531 - mae: 122.4189\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 62786.3281 - mae: 91.4700\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 103653.2031 - mae: 124.8948\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 123250.6641 - mae: 129.4874\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 77197.3125 - mae: 107.5544\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 48484.0234 - mae: 88.8686\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75070.7031 - mae: 105.9161\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85277.1406 - mae: 121.4515\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 78111.7969 - mae: 109.1351\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 94919.1406 - mae: 121.7524\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73173.6172 - mae: 109.5918\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 49504.9141 - mae: 83.4251\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 73661.9219 - mae: 99.6595\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 66438.1406 - mae: 100.5872\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 81745.5859 - mae: 109.5712\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 89926.1250 - mae: 113.8606\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 72803.9844 - mae: 110.0182\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 76567.4531 - mae: 117.6613\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 108833.0625 - mae: 128.4519\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43454.6172 - mae: 82.5921\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 86983.5625 - mae: 105.7121\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57140.3164 - mae: 86.7998\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63776.3281 - mae: 93.6307\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 75777.6641 - mae: 106.9757\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 70249.6484 - mae: 100.3230\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 93490.1484 - mae: 116.0184\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53011.9766 - mae: 91.8984\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 95419.0312 - mae: 121.4059\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 86041.5156 - mae: 109.0249\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38013.5547 - mae: 77.0930\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 54420.2500 - mae: 87.6567\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38600.3828 - mae: 81.6037\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61418.6914 - mae: 94.3812\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78165.7578 - mae: 104.1743\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 43065.2969 - mae: 79.9828\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 86680.9062 - mae: 113.0438\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 66139.8438 - mae: 99.7462\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 60661.0391 - mae: 86.6053\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 77792.8125 - mae: 114.4847\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 53689.0781 - mae: 89.9561\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33170.4219 - mae: 69.7859\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 83030.7969 - mae: 112.7608\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52497.5977 - mae: 84.8617\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57934.1016 - mae: 96.1951\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 67056.4062 - mae: 90.7412\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 90389.8047 - mae: 116.8138\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 65007.1562 - mae: 102.2714\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 47234.6875 - mae: 84.5168\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 51422.5859 - mae: 89.4423\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 46920.3438 - mae: 82.6496\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 59174.6211 - mae: 94.8437\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67568.8750 - mae: 101.6113\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 75490.8281 - mae: 112.8392\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 58468.4141 - mae: 92.2719\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51957.3594 - mae: 83.2708\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68775.5938 - mae: 100.1938\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40771.3398 - mae: 79.3748\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 64662.3008 - mae: 99.2437\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 63154.6250 - mae: 98.4390\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52802.1797 - mae: 88.9412\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 73840.1797 - mae: 99.7962\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 55505.3633 - mae: 88.6476\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35493.3984 - mae: 76.8554\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 66941.1016 - mae: 99.5876\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39257.5625 - mae: 82.2803\n",
      "training 0.9320653479069899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 106886.6484 - mae: 127.1971\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 53998.4766 - mae: 82.9575\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51959.6719 - mae: 88.1982\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 66854.3984 - mae: 102.2833\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 67796.5000 - mae: 100.0919\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 38190.5859 - mae: 75.7052\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 57060.7109 - mae: 85.7487\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 54217.9375 - mae: 86.2299\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 52794.7812 - mae: 94.4515\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 35113.0781 - mae: 71.7847\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39684.1406 - mae: 79.7628\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 53184.1836 - mae: 91.3190\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46524.8047 - mae: 85.2229\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 57972.3320 - mae: 96.1622\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 62242.8828 - mae: 97.7547\n",
      "training 0.9320653479069899\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 41916.8672 - mae: 83.0208\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34654.7891 - mae: 68.0225\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26927.7051 - mae: 61.3834\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30561.9141 - mae: 71.0264\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 49392.7031 - mae: 86.5113\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 48861.3125 - mae: 86.7011\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32739.1797 - mae: 76.0470\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 27254.5977 - mae: 68.1789\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53680.4766 - mae: 91.8266\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 45893.2383 - mae: 85.1600\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 80787.2578 - mae: 110.2829\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 42899.3359 - mae: 80.2403\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 57143.6797 - mae: 93.2405\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 26246.4629 - mae: 61.2304\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 54553.0938 - mae: 87.0893\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 47446.8438 - mae: 88.8302\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33964.5547 - mae: 67.4127\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61126.8242 - mae: 89.8080\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 52718.6602 - mae: 88.6285\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 43962.0273 - mae: 75.9708\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27423.1094 - mae: 63.8051\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42687.5742 - mae: 69.9995\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42495.2734 - mae: 84.7979\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 52930.3828 - mae: 93.2523\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44806.7500 - mae: 76.6919\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 38309.2930 - mae: 73.8328\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 46785.0781 - mae: 82.6019\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37622.7266 - mae: 70.0030\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44965.2500 - mae: 84.0806\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31183.8750 - mae: 71.0229\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 84707.1484 - mae: 107.1701\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 44030.0664 - mae: 83.0959\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30073.4160 - mae: 66.1677\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 37614.4727 - mae: 73.0495\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44262.0547 - mae: 79.3853\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50023.3828 - mae: 87.4917\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35945.5000 - mae: 74.9588\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 49869.7500 - mae: 79.4783\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56403.4531 - mae: 94.4217\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33757.1875 - mae: 67.3698\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32924.2500 - mae: 69.8332\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37090.6914 - mae: 74.3855\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 42137.0312 - mae: 76.0380\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 56115.1484 - mae: 92.2919\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 28144.1328 - mae: 60.3239\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 44097.9023 - mae: 85.0219\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25087.4277 - mae: 59.4445\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 55458.5742 - mae: 89.6602\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 41524.4609 - mae: 76.6097\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50047.9375 - mae: 81.9214\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48673.5195 - mae: 80.9325\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34486.9023 - mae: 67.3221\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38234.4688 - mae: 78.0494\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 57715.9023 - mae: 89.3349\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 23319.6211 - mae: 57.2358\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 30096.4766 - mae: 64.8388\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33634.2656 - mae: 71.6336\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 22687.6621 - mae: 55.5724\n",
      "training 0.92274469442792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 57236.8516 - mae: 86.1771\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 53535.9844 - mae: 92.5510\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25681.5254 - mae: 64.3504\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34322.2891 - mae: 71.4911\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34509.4609 - mae: 71.3146\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33720.2305 - mae: 66.1589\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40705.1406 - mae: 75.5644\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 30908.1680 - mae: 68.5310\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22174.8535 - mae: 61.1582\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 31596.0391 - mae: 72.9939\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20050.0000 - mae: 50.5976\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 50974.9453 - mae: 84.4559\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 27581.7949 - mae: 64.0313\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21143.0664 - mae: 56.7843\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 23679.3242 - mae: 58.6163\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29923.9473 - mae: 63.0420\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34312.7188 - mae: 67.8371\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29394.7344 - mae: 72.6611\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 33177.5703 - mae: 76.2356\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 58425.7109 - mae: 93.6358\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27523.3770 - mae: 68.0902\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 27315.5586 - mae: 65.8046\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 25595.7129 - mae: 59.4019\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33159.3789 - mae: 71.2263\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 35977.1328 - mae: 71.9119\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25435.7930 - mae: 60.5890\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33730.5938 - mae: 71.6710\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36558.7656 - mae: 72.9992\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 38820.0352 - mae: 75.8391\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 30904.7754 - mae: 70.6487\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 35550.6016 - mae: 76.5802\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18003.4609 - mae: 48.2745\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 26138.8320 - mae: 64.5089\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24035.7109 - mae: 58.9251\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 29794.6250 - mae: 64.5438\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28023.9902 - mae: 60.9166\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 25616.1172 - mae: 63.7854\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42455.7656 - mae: 81.5802\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 46044.3477 - mae: 84.5642\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 33083.6094 - mae: 68.9424\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 30565.6953 - mae: 60.3247\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 25206.6406 - mae: 62.4887\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26004.9316 - mae: 59.0421\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25049.1289 - mae: 57.4724\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 37816.0508 - mae: 79.1278\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27405.4004 - mae: 62.7438\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22716.5234 - mae: 56.7867\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33853.4922 - mae: 70.4976\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39687.8320 - mae: 74.5156\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21079.8867 - mae: 55.8565\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42194.6367 - mae: 70.3425\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29485.9141 - mae: 67.4313\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25234.6719 - mae: 61.4504\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23214.7422 - mae: 60.2396\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17979.1504 - mae: 50.4760\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 39790.3281 - mae: 78.3456\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 20875.7578 - mae: 52.7616\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 25963.1660 - mae: 60.9879\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21712.4922 - mae: 58.1488\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23864.0645 - mae: 57.5793\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41567.0664 - mae: 75.5917\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 32110.0078 - mae: 66.2719\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23577.1934 - mae: 61.2609\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 21990.4668 - mae: 55.3044\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30945.7480 - mae: 67.0682\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34924.6445 - mae: 68.4083\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19267.5332 - mae: 52.7744\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26767.8906 - mae: 65.8230\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29809.8164 - mae: 64.1851\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 40254.6055 - mae: 81.2830\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 28474.7754 - mae: 65.6768\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22707.8203 - mae: 53.8347\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23267.0195 - mae: 60.1846\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16829.6055 - mae: 48.4110\n",
      "training 0.92274469442792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 26128.5703 - mae: 61.4449\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 30222.2070 - mae: 69.1933\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18374.4551 - mae: 52.8229\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26431.0742 - mae: 61.2259\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 40829.4609 - mae: 70.3014\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22804.2461 - mae: 54.5530\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18365.9961 - mae: 49.8359\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33081.4844 - mae: 74.1888\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19717.1797 - mae: 58.7450\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20128.5352 - mae: 52.9887\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26522.9238 - mae: 58.5258\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15808.5186 - mae: 49.9746\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34563.2539 - mae: 67.8069\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 34928.6328 - mae: 75.6310\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 35512.1797 - mae: 67.6063\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33848.3750 - mae: 66.9264\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 29652.0781 - mae: 62.2485\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 26164.1406 - mae: 65.0839\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 23919.3633 - mae: 59.1479\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26166.6953 - mae: 62.2232\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 29995.9785 - mae: 63.4647\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30543.0586 - mae: 62.1653\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19267.6504 - mae: 55.2237\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 30919.6270 - mae: 70.1112\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 21611.3438 - mae: 58.4207\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 48739.1172 - mae: 78.7657\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21636.4277 - mae: 53.4779\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26334.5234 - mae: 61.9330\n",
      "training 0.92274469442792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23660.3027 - mae: 56.7924\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42451.7500 - mae: 69.9793\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 16905.7227 - mae: 51.5467\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 32823.2500 - mae: 68.9201\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 20400.9512 - mae: 58.6377\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17454.2305 - mae: 49.8148\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14026.9785 - mae: 43.4691\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 47435.9297 - mae: 81.5459\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35878.8750 - mae: 76.6491\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 23482.5508 - mae: 61.6164\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 43317.8242 - mae: 80.5238\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 42049.6211 - mae: 79.9583\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 25867.5176 - mae: 64.2492\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 38788.9453 - mae: 73.8334\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 28321.4766 - mae: 60.7268\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20177.5391 - mae: 54.1510\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29557.0645 - mae: 64.6623\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18250.2930 - mae: 49.0505\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23722.5977 - mae: 56.8997\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12410.0977 - mae: 43.4787\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16617.0117 - mae: 47.7908\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15472.6709 - mae: 48.1905\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 27321.9297 - mae: 64.5383\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18111.0273 - mae: 54.2574\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18469.1055 - mae: 53.2191\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21863.0352 - mae: 56.0268\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21072.8242 - mae: 55.7999\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22803.1680 - mae: 60.1227\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26207.1973 - mae: 59.9311\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20505.0430 - mae: 58.9626\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24551.2480 - mae: 58.7323\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25441.2480 - mae: 61.6459\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27269.4766 - mae: 67.6741\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27192.6836 - mae: 61.5371\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13195.5469 - mae: 45.8178\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14968.3555 - mae: 45.8905\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22277.1914 - mae: 56.4875\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36981.3125 - mae: 73.9406\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23052.0781 - mae: 60.1507\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23092.2812 - mae: 61.1101\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 31777.2461 - mae: 66.0862\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24025.0742 - mae: 62.2881\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27631.1113 - mae: 60.7159\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13067.1553 - mae: 40.6101\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17375.9531 - mae: 48.5023\n",
      "training 0.9135172474836407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 33510.2500 - mae: 70.3022\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14456.8711 - mae: 45.1073\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23761.0000 - mae: 60.4231\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24513.5430 - mae: 63.1280\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16781.5547 - mae: 49.0312\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28790.8613 - mae: 67.0694\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15407.8154 - mae: 48.4058\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14748.6230 - mae: 47.5508\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 25113.2383 - mae: 60.3813\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17344.5781 - mae: 50.6039\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22367.1465 - mae: 55.4186\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12747.7949 - mae: 40.9105\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 32148.0078 - mae: 65.3979\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15567.6143 - mae: 50.4549\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25868.4570 - mae: 57.1721\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23861.6523 - mae: 61.0047\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24757.9922 - mae: 53.2178\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15436.1387 - mae: 44.5071\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13876.5254 - mae: 47.8891\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22031.3555 - mae: 58.5747\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15054.0410 - mae: 47.9021\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27609.3516 - mae: 61.0391\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 31417.3906 - mae: 67.8588\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10076.7246 - mae: 38.8812\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 978us/step - loss: 23848.7402 - mae: 57.0622\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29691.6621 - mae: 66.8695\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14266.0205 - mae: 46.2348\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23097.0586 - mae: 56.9440\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13490.4648 - mae: 46.0233\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27026.9941 - mae: 61.7170\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11506.7910 - mae: 38.2627\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17645.1543 - mae: 48.3802\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 18119.0703 - mae: 49.7963\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26458.5938 - mae: 60.2535\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12935.0137 - mae: 45.8997\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16288.1318 - mae: 49.7684\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14272.4707 - mae: 45.8028\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24523.2715 - mae: 58.7256\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23673.7598 - mae: 57.6098\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13170.5840 - mae: 43.7081\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27778.7031 - mae: 64.8587\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 32385.1914 - mae: 67.1237\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14289.9297 - mae: 41.8211\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9009.9600 - mae: 35.7699\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 17295.5898 - mae: 50.6465\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21120.6836 - mae: 53.1887\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13518.8770 - mae: 43.0925\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11507.0352 - mae: 39.1699\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22075.9453 - mae: 58.5930\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21872.6719 - mae: 58.8006\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24461.2773 - mae: 57.8938\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11104.3965 - mae: 41.4183\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13799.0059 - mae: 46.5203\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19320.0078 - mae: 52.6334\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 17140.8145 - mae: 47.9716\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16677.3867 - mae: 46.2156\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 18488.2969 - mae: 52.4023\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21588.7090 - mae: 54.3461\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14910.3662 - mae: 49.1668\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16608.0957 - mae: 50.7705\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16682.7871 - mae: 48.5103\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16982.1484 - mae: 50.3330\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17699.2559 - mae: 49.5456\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11543.0859 - mae: 41.9700\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9722.6621 - mae: 38.8630\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14129.4443 - mae: 43.5317\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18220.0137 - mae: 50.6547\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14599.9902 - mae: 46.4505\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16718.2363 - mae: 43.8890\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11516.2939 - mae: 39.6588\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22508.5254 - mae: 52.5391\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16757.9434 - mae: 50.5199\n",
      "training 0.9135172474836407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 11418.8252 - mae: 41.7766\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12371.2686 - mae: 45.1991\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26206.8672 - mae: 62.8901\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10577.6445 - mae: 38.3536\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 14700.0654 - mae: 45.2505\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27223.5723 - mae: 65.1341\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13188.8574 - mae: 48.8095\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14190.1006 - mae: 46.2760\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11111.4248 - mae: 38.1927\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15772.6953 - mae: 48.5879\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8151.2378 - mae: 35.1038\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19519.7500 - mae: 55.4390\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17303.9375 - mae: 50.6289\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21504.5703 - mae: 60.0017\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23815.8477 - mae: 63.0634\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18178.3672 - mae: 48.4592\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17272.0879 - mae: 49.3420\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 23370.1406 - mae: 56.2658\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16668.0332 - mae: 50.0502\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15985.6895 - mae: 49.4568\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16558.9707 - mae: 46.5337\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12757.9316 - mae: 40.3303\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18328.0430 - mae: 54.5193\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 18441.0742 - mae: 48.6417\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12291.3877 - mae: 42.6443\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 32588.7266 - mae: 68.0205\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10379.0303 - mae: 37.6706\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16249.4141 - mae: 48.7505\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20273.6758 - mae: 52.5682\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16461.4102 - mae: 45.5358\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29782.3848 - mae: 65.0072\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13413.8672 - mae: 41.3692\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 28098.9062 - mae: 60.5071\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16731.9453 - mae: 48.3777\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15294.9844 - mae: 45.0919\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9550.5410 - mae: 36.0350\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5205.6221 - mae: 28.2157\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12763.7402 - mae: 45.9988\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16747.5430 - mae: 48.3517\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20828.1035 - mae: 58.3349\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13184.3770 - mae: 44.5722\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7825.9229 - mae: 32.7957\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15195.3877 - mae: 46.4101\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18421.7754 - mae: 45.9794\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8371.1855 - mae: 36.6786\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16747.9023 - mae: 52.5785\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10728.7070 - mae: 37.3954\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17088.6895 - mae: 48.0365\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 24722.8496 - mae: 61.4455\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19892.0098 - mae: 53.7998\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15288.2012 - mae: 45.0552\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18699.5762 - mae: 49.9254\n",
      "training 0.9135172474836407\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15759.8633 - mae: 43.3280\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14427.4824 - mae: 45.6489\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18012.3379 - mae: 53.8466\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16613.5586 - mae: 48.5544\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18127.5547 - mae: 51.1970\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16281.5273 - mae: 50.7815\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11345.2129 - mae: 40.1712\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15991.4375 - mae: 48.5700\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20078.7617 - mae: 54.2118\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11092.6826 - mae: 41.1079\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13723.1279 - mae: 47.5207\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14295.6152 - mae: 45.2756\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13360.6855 - mae: 45.9107\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14555.1855 - mae: 42.9174\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14827.0752 - mae: 45.6072\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14115.0273 - mae: 48.0909\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13735.5059 - mae: 45.9815\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20995.8848 - mae: 56.3851\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14254.1133 - mae: 46.3139\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14761.0771 - mae: 48.3462\n",
      "training 0.9043820750088043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 8364.0508 - mae: 39.0431\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10141.7285 - mae: 38.2036\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9520.0781 - mae: 40.3446\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16157.5566 - mae: 49.2609\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16330.3652 - mae: 50.7711\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13747.8516 - mae: 46.6903\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12855.1543 - mae: 45.6357\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8948.4863 - mae: 35.2255\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16065.6211 - mae: 43.7819\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12594.5684 - mae: 41.7428\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11468.2168 - mae: 39.7151\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14825.0527 - mae: 45.7778\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8194.9766 - mae: 35.9171\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16209.2773 - mae: 50.3907\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15359.9736 - mae: 47.7684\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9310.7627 - mae: 38.0803\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19305.2031 - mae: 53.0506\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18597.1035 - mae: 54.6872\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15747.7812 - mae: 49.3917\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20820.8320 - mae: 56.6301\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15645.4756 - mae: 48.2983\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15707.7598 - mae: 50.4561\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17411.1582 - mae: 47.9314\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14598.6484 - mae: 43.0727\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16991.2832 - mae: 47.8784\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8262.6973 - mae: 36.8332\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10435.7246 - mae: 36.7780\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12327.4326 - mae: 41.5860\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13611.9932 - mae: 49.4793\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12014.5254 - mae: 41.5639\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12169.6719 - mae: 41.3177\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11822.3955 - mae: 36.0228\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14790.7764 - mae: 45.6723\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13625.0674 - mae: 47.4442\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9888.2705 - mae: 37.8044\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17778.9551 - mae: 51.3151\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8097.1416 - mae: 34.7149\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14373.4629 - mae: 44.8931\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14221.7656 - mae: 45.1701\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13551.4688 - mae: 47.3551\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12873.3203 - mae: 43.8494\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10770.9443 - mae: 39.7763\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9811.7266 - mae: 36.5966\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18391.1270 - mae: 52.7251\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10572.2109 - mae: 39.3390\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16873.2031 - mae: 50.4262\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12922.7822 - mae: 45.9347\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8273.3730 - mae: 34.6727\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19251.4902 - mae: 52.1751\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14645.0918 - mae: 47.3919\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12373.2734 - mae: 45.7957\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11604.1592 - mae: 41.8661\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16518.6406 - mae: 50.3906\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13325.4805 - mae: 40.6458\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10380.7900 - mae: 38.0720\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9835.5938 - mae: 37.4970\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14547.4766 - mae: 41.4869\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13419.9199 - mae: 43.8064\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14974.7422 - mae: 47.5199\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13717.3906 - mae: 47.0457\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8544.4541 - mae: 34.9656\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16567.2695 - mae: 48.1225\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9639.5596 - mae: 38.6437\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11984.1465 - mae: 39.8271\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10005.8301 - mae: 39.6040\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9623.7568 - mae: 39.9709\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12471.1982 - mae: 41.8808\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11533.8047 - mae: 39.1799\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9730.6719 - mae: 37.4233\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6890.8457 - mae: 33.0903\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13508.0996 - mae: 42.6148\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17701.3359 - mae: 53.0103\n",
      "training 0.9043820750088043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 21482.6641 - mae: 51.6788\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14660.8457 - mae: 47.9927\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18528.4375 - mae: 50.1081\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10027.1748 - mae: 38.9486\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9423.8418 - mae: 38.4830\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13199.9395 - mae: 41.1631\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20074.7910 - mae: 55.5797\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15719.2529 - mae: 47.1045\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12559.5117 - mae: 42.7374\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10331.8428 - mae: 36.0109\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9826.4258 - mae: 35.5881\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7961.4092 - mae: 32.9681\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8218.5332 - mae: 37.0783\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7194.1992 - mae: 33.8721\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 15890.9854 - mae: 48.8996\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9533.5537 - mae: 40.6598\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13099.9062 - mae: 45.7395\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11270.0527 - mae: 42.4213\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7975.4702 - mae: 34.9865\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13008.0322 - mae: 41.6556\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13318.3008 - mae: 42.2551\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9041.4512 - mae: 40.4965\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16307.4609 - mae: 51.6308\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8943.9805 - mae: 36.1535\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14225.7217 - mae: 47.7608\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11031.2871 - mae: 41.1873\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14793.0000 - mae: 47.1833\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10233.7949 - mae: 38.1073\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17712.5625 - mae: 51.0302\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6346.5586 - mae: 30.4046\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12086.7021 - mae: 44.2384\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14106.6914 - mae: 45.5721\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11408.9434 - mae: 41.1597\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11023.0166 - mae: 39.4950\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14559.0840 - mae: 47.0129\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11433.8672 - mae: 43.3824\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14599.4727 - mae: 47.2638\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12883.3887 - mae: 44.6845\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10446.0088 - mae: 37.4338\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16232.5098 - mae: 50.2577\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8582.8008 - mae: 33.8204\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11421.8701 - mae: 43.3084\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11158.1357 - mae: 39.2596\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12814.6875 - mae: 39.3821\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12696.9551 - mae: 42.9951\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12904.5947 - mae: 44.9180\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17474.2793 - mae: 51.1571\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10933.4160 - mae: 39.6880\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6448.1729 - mae: 32.1215\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10224.2070 - mae: 40.3484\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6523.5605 - mae: 31.6783\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14059.5059 - mae: 41.3731\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9591.8135 - mae: 37.7209\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17023.7871 - mae: 52.9214\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19752.3379 - mae: 52.8882\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13512.0527 - mae: 46.2967\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8000.9263 - mae: 35.4029\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13112.5000 - mae: 42.7282\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9247.3457 - mae: 35.5005\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8859.1602 - mae: 37.3417\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9504.7432 - mae: 39.4539\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13784.8350 - mae: 43.2334\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10650.9922 - mae: 41.4713\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14840.8027 - mae: 43.2076\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8972.0488 - mae: 33.9897\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11342.7080 - mae: 42.4242\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8973.8262 - mae: 37.2181\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11286.9980 - mae: 40.3495\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11592.8398 - mae: 39.1066\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18347.4766 - mae: 51.1879\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9680.7363 - mae: 37.3311\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11817.4863 - mae: 44.1673\n",
      "training 0.9043820750088043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 9136.1670 - mae: 36.6725\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9726.2324 - mae: 39.3816\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8684.7471 - mae: 36.4343\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9170.9043 - mae: 35.6014\n",
      "training 0.9043820750088043\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11104.9648 - mae: 40.8755\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11128.4912 - mae: 39.4487\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8569.7627 - mae: 35.7509\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8783.7461 - mae: 37.0495\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13212.4883 - mae: 43.3315\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15182.0586 - mae: 48.2841\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10394.9854 - mae: 39.9582\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11321.3701 - mae: 41.5034\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4732.0195 - mae: 28.1490\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8087.2500 - mae: 37.0906\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8996.8984 - mae: 33.8308\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 14607.0342 - mae: 43.7367\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8055.0342 - mae: 35.5569\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6425.8145 - mae: 32.3220\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7043.3911 - mae: 31.7328\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 14119.8574 - mae: 46.6156\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8218.1826 - mae: 35.2124\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12505.6973 - mae: 42.3361\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11341.3076 - mae: 42.4883\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 16520.2578 - mae: 48.8424\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15372.2734 - mae: 45.0563\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11949.2686 - mae: 37.3968\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8644.1104 - mae: 34.8961\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7893.4229 - mae: 34.6412\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11024.3447 - mae: 43.6276\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14710.7773 - mae: 46.4401\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7830.3442 - mae: 32.8512\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8421.1895 - mae: 37.5122\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12378.6348 - mae: 40.5399\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8660.1113 - mae: 36.3982\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10121.4170 - mae: 34.9837\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12221.9941 - mae: 42.6895\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10779.9160 - mae: 40.5880\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12772.4951 - mae: 42.9705\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10458.5098 - mae: 39.1917\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8427.8027 - mae: 36.4557\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8038.4536 - mae: 34.7310\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5646.3975 - mae: 29.4798\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7847.2939 - mae: 33.6781\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9095.4814 - mae: 37.8319\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6552.5449 - mae: 31.2841\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9742.8555 - mae: 36.0933\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9598.3760 - mae: 33.9247\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10046.8008 - mae: 38.8717\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6116.3555 - mae: 31.4039\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13491.0088 - mae: 42.3637\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11303.7227 - mae: 41.3256\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8147.0596 - mae: 38.2075\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14433.0566 - mae: 45.3769\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10921.5430 - mae: 37.2076\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8591.6846 - mae: 35.3517\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9033.9854 - mae: 38.5773\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12938.6211 - mae: 43.4623\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11969.5918 - mae: 41.8045\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12438.7129 - mae: 42.3326\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11803.1445 - mae: 40.5236\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14196.6953 - mae: 45.8850\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5971.6230 - mae: 30.0557\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10955.3535 - mae: 40.4362\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11482.8057 - mae: 40.1561\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12598.7324 - mae: 43.1922\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5618.1387 - mae: 27.8138\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10139.2529 - mae: 39.1736\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10329.6602 - mae: 41.2720\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15180.6172 - mae: 49.3006\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10960.9531 - mae: 42.3514\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8294.6328 - mae: 36.3629\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8833.7559 - mae: 36.8175\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10383.5791 - mae: 39.2627\n",
      "training 0.8953382542587163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 8628.8906 - mae: 37.1449\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13120.7227 - mae: 46.0546\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9974.4150 - mae: 38.5917\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11914.5215 - mae: 42.3893\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8659.5098 - mae: 36.1828\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10568.1855 - mae: 39.4247\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14234.5215 - mae: 40.8479\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6900.4399 - mae: 32.5489\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8235.1689 - mae: 35.6241\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10766.6816 - mae: 39.6023\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13229.4668 - mae: 46.4184\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11585.5098 - mae: 37.1794\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13381.6738 - mae: 46.6205\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6826.5283 - mae: 32.4057\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8303.4258 - mae: 35.0505\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9758.9688 - mae: 38.5779\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9287.6641 - mae: 38.4112\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10616.8984 - mae: 39.1145\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12374.4121 - mae: 41.2580\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9058.6426 - mae: 36.3271\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6678.0381 - mae: 31.5896\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10855.6104 - mae: 39.7484\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10735.6523 - mae: 39.4201\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8894.0361 - mae: 33.9551\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13584.5449 - mae: 44.8794\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7635.5254 - mae: 32.7503\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8280.9375 - mae: 32.2414\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14091.9043 - mae: 43.9140\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12007.7422 - mae: 41.1320\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9428.3516 - mae: 37.5811\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11516.9961 - mae: 42.1351\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9424.2783 - mae: 36.3079\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6764.1045 - mae: 32.4749\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10512.8809 - mae: 40.0152\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7739.5869 - mae: 34.0883\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6566.9170 - mae: 28.6197\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15197.8242 - mae: 51.2031\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8457.9590 - mae: 35.3789\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10306.2559 - mae: 38.4286\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10678.8770 - mae: 41.8825\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10699.3008 - mae: 36.8265\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6506.7705 - mae: 30.9455\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7443.7593 - mae: 34.9569\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8793.3164 - mae: 35.8973\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9709.3320 - mae: 38.1805\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6821.4272 - mae: 32.0109\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8609.3145 - mae: 35.2441\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9084.9170 - mae: 38.4252\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7950.3564 - mae: 35.3578\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8600.0098 - mae: 36.3779\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8016.1191 - mae: 33.4773\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5757.3838 - mae: 28.2356\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9133.6152 - mae: 34.7969\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12833.6992 - mae: 44.0953\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9503.1631 - mae: 38.9480\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6048.7051 - mae: 29.6160\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7878.5371 - mae: 33.6631\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4358.8594 - mae: 27.1503\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9277.1338 - mae: 34.2718\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8515.6602 - mae: 37.8164\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11924.7324 - mae: 40.4816\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12842.5918 - mae: 43.0358\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5434.5464 - mae: 28.7280\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9296.5479 - mae: 36.3757\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8344.5879 - mae: 33.9833\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5858.3135 - mae: 28.9094\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9371.9824 - mae: 32.0843\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7206.2686 - mae: 33.7868\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9280.5107 - mae: 35.5289\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9899.6377 - mae: 38.6139\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7900.2246 - mae: 35.7803\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11100.6621 - mae: 38.6535\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8961.4961 - mae: 35.5455\n",
      "training 0.8953382542587163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 7311.2661 - mae: 31.4280\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5264.8369 - mae: 26.8900\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10841.3438 - mae: 39.1523\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8555.4639 - mae: 31.6020\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7392.8184 - mae: 30.9179\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6792.0469 - mae: 33.2698\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11045.1348 - mae: 40.1057\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10060.4043 - mae: 37.3508\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7665.8335 - mae: 33.2196\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10121.2617 - mae: 39.0870\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10147.3379 - mae: 37.1833\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8136.9819 - mae: 33.3156\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5650.1162 - mae: 28.4106\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8624.5410 - mae: 34.8389\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4577.9688 - mae: 26.8295\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6005.5693 - mae: 29.1093\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12777.2891 - mae: 44.7080\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4178.5928 - mae: 27.1952\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6001.6743 - mae: 29.2925\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12312.1855 - mae: 40.1547\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8623.0039 - mae: 35.1563\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5329.5801 - mae: 29.6712\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8531.6113 - mae: 36.0636\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8075.3569 - mae: 35.0391\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5720.1890 - mae: 29.3969\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6843.2930 - mae: 29.9304\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5012.6084 - mae: 27.8468\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4585.9424 - mae: 26.0757\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5812.8887 - mae: 27.0454\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7408.7725 - mae: 33.7875\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8240.5508 - mae: 34.3565\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8257.8965 - mae: 35.6058\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4517.4692 - mae: 25.3872\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5415.8071 - mae: 28.2734\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6151.3936 - mae: 29.3644\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6085.8623 - mae: 29.5568\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4558.0039 - mae: 26.6534\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4982.6177 - mae: 26.4291\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8158.2783 - mae: 34.6559\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10961.0391 - mae: 39.9453\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6647.0684 - mae: 31.4670\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7720.4292 - mae: 33.3641\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8108.8457 - mae: 34.3835\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8286.3633 - mae: 35.9837\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7068.5991 - mae: 31.6191\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9275.1592 - mae: 37.3703\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6104.6592 - mae: 29.6359\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4009.2173 - mae: 25.9022\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7694.8599 - mae: 34.3541\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5717.9170 - mae: 29.2076\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6123.8701 - mae: 30.1097\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8308.9287 - mae: 34.3428\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9910.6523 - mae: 40.1490\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9676.5957 - mae: 37.3871\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4205.5391 - mae: 24.5404\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7101.5249 - mae: 27.4816\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4865.2578 - mae: 26.0014\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3413.7085 - mae: 23.9250\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6389.5425 - mae: 29.7418\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8622.8438 - mae: 35.1690\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6811.2949 - mae: 31.1718\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5829.1929 - mae: 28.5859\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5953.4365 - mae: 30.4451\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6787.2969 - mae: 32.0653\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10240.5605 - mae: 35.5392\n",
      "training 0.8953382542587163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5844.0039 - mae: 29.5978\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7676.4268 - mae: 34.1833\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7343.1465 - mae: 33.5389\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5855.6035 - mae: 29.8425\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9764.8496 - mae: 37.1333\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8612.9365 - mae: 33.0765\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4978.1338 - mae: 26.7378\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5280.6611 - mae: 25.6045\n",
      "training 0.8863848717161291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 7261.7461 - mae: 32.0119\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6147.3174 - mae: 25.1274\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5641.7959 - mae: 29.5530\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4996.5996 - mae: 26.2804\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10286.3584 - mae: 39.2684\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8263.4297 - mae: 35.5724\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9064.0898 - mae: 38.1505\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5158.7031 - mae: 27.4736\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5934.2153 - mae: 27.9483\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7335.7925 - mae: 32.5223\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4041.5645 - mae: 24.2492\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5595.4014 - mae: 28.9216\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6116.6797 - mae: 29.2267\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2675.7490 - mae: 20.6025\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7393.1436 - mae: 34.5914\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7836.3555 - mae: 34.7408\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8073.1528 - mae: 33.8039\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7212.1924 - mae: 32.8233\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5372.2373 - mae: 26.9909\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6315.6221 - mae: 31.4710\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5108.6064 - mae: 28.6863\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6609.7485 - mae: 31.1777\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9937.4922 - mae: 35.8010\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3827.7910 - mae: 24.7049\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5526.7729 - mae: 27.2950\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7566.7783 - mae: 34.6730\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5234.8486 - mae: 27.1607\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5776.3350 - mae: 27.5756\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3830.0903 - mae: 23.4655\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7574.3003 - mae: 34.5743\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5979.3926 - mae: 31.1409\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7599.0161 - mae: 32.5222\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6184.6855 - mae: 29.5399\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7205.3066 - mae: 31.6313\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6640.2710 - mae: 31.2454\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4809.4365 - mae: 26.4826\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5796.8389 - mae: 27.0924\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4937.1465 - mae: 27.5156\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7938.8369 - mae: 32.8352\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4483.6367 - mae: 25.2026\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6346.7061 - mae: 31.9014\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5156.2422 - mae: 27.2948\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5796.0098 - mae: 30.9301\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6178.9971 - mae: 28.6500\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6962.5552 - mae: 32.6105\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4919.0918 - mae: 28.0763\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3230.6431 - mae: 21.0628\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5458.5269 - mae: 27.6917\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5788.0244 - mae: 26.8722\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6634.0933 - mae: 31.6326\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5971.0200 - mae: 29.0762\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4307.3638 - mae: 26.3982\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4174.3462 - mae: 23.9969\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6643.6914 - mae: 29.7735\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7883.0547 - mae: 32.3766\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3631.6045 - mae: 22.3476\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5199.1484 - mae: 29.1270\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4178.6089 - mae: 24.2679\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7142.8486 - mae: 32.3001\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5647.8369 - mae: 29.1690\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6291.0928 - mae: 29.1953\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7209.4648 - mae: 33.2207\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4481.0806 - mae: 23.9911\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3688.8027 - mae: 22.7716\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7033.8921 - mae: 33.0436\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4535.3203 - mae: 25.6655\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10289.0020 - mae: 39.3828\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5343.8091 - mae: 27.0266\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5998.9209 - mae: 29.0956\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6707.4160 - mae: 30.1294\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4678.7729 - mae: 25.9325\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5552.8403 - mae: 29.7755\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6853.2490 - mae: 31.8176\n",
      "training 0.8863848717161291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 6445.3281 - mae: 29.3810\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3657.8933 - mae: 23.3564\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5934.3086 - mae: 30.7215\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5584.6592 - mae: 28.6815\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7678.1191 - mae: 32.9970\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6812.7402 - mae: 31.5499\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5631.5791 - mae: 27.2601\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5245.6260 - mae: 28.5833\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6274.6309 - mae: 30.8433\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4996.8125 - mae: 29.4259\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4774.1504 - mae: 27.5024\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4929.0566 - mae: 27.4372\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2575.0022 - mae: 19.2472\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6153.5986 - mae: 30.2980\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6075.5059 - mae: 27.9038\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4521.5957 - mae: 25.7658\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7955.2710 - mae: 34.9726\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7473.1328 - mae: 34.7380\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9238.5879 - mae: 38.4783\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2246.8989 - mae: 17.8924\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5102.6348 - mae: 26.3970\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5153.9746 - mae: 25.2856\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4703.8247 - mae: 27.5930\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6414.9834 - mae: 29.0969\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5174.1055 - mae: 29.5872\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5055.2451 - mae: 27.3357\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6347.0762 - mae: 27.7409\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4203.9092 - mae: 24.0373\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3900.0186 - mae: 24.5457\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5050.7134 - mae: 26.3136\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3086.1792 - mae: 22.0280\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4914.5381 - mae: 25.8888\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4949.7021 - mae: 26.4598\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4575.3120 - mae: 26.8566\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6217.7412 - mae: 30.4152\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3538.4109 - mae: 23.7493\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4739.8296 - mae: 25.7971\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5321.2881 - mae: 29.0036\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4300.8818 - mae: 25.6251\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3289.4531 - mae: 21.5992\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5359.8555 - mae: 26.7052\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4657.1494 - mae: 24.2243\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4876.4912 - mae: 26.8143\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8036.0308 - mae: 33.4997\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3776.3203 - mae: 23.4499\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3884.3730 - mae: 24.2994\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5996.2197 - mae: 29.1271\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3785.4617 - mae: 23.8824\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5440.1338 - mae: 26.8096\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3973.7402 - mae: 24.4115\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5164.9224 - mae: 27.5092\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5379.3516 - mae: 28.7982\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8186.1836 - mae: 36.7274\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4671.4619 - mae: 25.9045\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7230.8555 - mae: 31.7771\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5974.2441 - mae: 31.3970\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6814.4043 - mae: 31.5024\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4113.0059 - mae: 23.7839\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7035.1782 - mae: 30.5924\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6683.6943 - mae: 30.3001\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6160.9673 - mae: 29.3567\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5663.7749 - mae: 27.7975\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3857.7014 - mae: 24.8067\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6165.6934 - mae: 30.8053\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5463.9009 - mae: 28.7150\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4508.8726 - mae: 25.3116\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5262.3457 - mae: 28.5303\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4486.3955 - mae: 27.2212\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6293.1543 - mae: 28.9787\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4511.6655 - mae: 26.0785\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7374.4487 - mae: 32.3921\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5176.5601 - mae: 29.4400\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4141.8398 - mae: 25.1212\n",
      "training 0.8863848717161291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 3544.0056 - mae: 23.5489\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4780.2510 - mae: 26.7520\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2815.5674 - mae: 21.1102\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6983.8535 - mae: 32.9953\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5656.7256 - mae: 27.1097\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6334.3047 - mae: 30.2643\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3603.1704 - mae: 23.5357\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2339.8318 - mae: 18.9466\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4672.3291 - mae: 26.9685\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4883.2026 - mae: 25.2776\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6480.0205 - mae: 31.7522\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4389.1821 - mae: 26.1796\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6712.2773 - mae: 30.4812\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5015.3125 - mae: 28.2867\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5377.5405 - mae: 28.0904\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3215.3740 - mae: 22.4358\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1649.1210 - mae: 15.2684\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4738.7222 - mae: 26.3864\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6579.1196 - mae: 28.8218\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5297.0781 - mae: 29.4580\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3632.5740 - mae: 21.9860\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6142.0884 - mae: 30.0219\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6512.5537 - mae: 31.4889\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7067.6738 - mae: 31.7851\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5651.8984 - mae: 29.7903\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5545.8008 - mae: 29.1569\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6036.5664 - mae: 31.0542\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5073.0771 - mae: 25.6768\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5938.7568 - mae: 29.1356\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5882.2744 - mae: 30.0781\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4661.3804 - mae: 27.4479\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4231.0918 - mae: 25.7751\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3131.3064 - mae: 22.0635\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6023.6553 - mae: 30.0702\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5562.0659 - mae: 29.0402\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6188.5967 - mae: 30.5226\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4272.8408 - mae: 26.5805\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4256.0278 - mae: 24.6949\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4746.6963 - mae: 28.3212\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4435.3115 - mae: 27.2689\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3269.0259 - mae: 21.7855\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3217.8076 - mae: 22.5639\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5151.4268 - mae: 26.9689\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3954.2439 - mae: 26.5614\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5172.8433 - mae: 28.0780\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5380.8984 - mae: 27.2177\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7168.1104 - mae: 30.3413\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4328.4897 - mae: 25.9140\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2516.3938 - mae: 19.8535\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2850.3479 - mae: 20.8276\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5508.8735 - mae: 26.4198\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3668.9919 - mae: 21.4325\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6526.3369 - mae: 29.0415\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7146.1436 - mae: 32.6839\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4907.6597 - mae: 28.8671\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5345.0425 - mae: 27.1080\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6742.7563 - mae: 31.0150\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4319.8027 - mae: 25.0126\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2349.8030 - mae: 19.8585\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3323.2642 - mae: 21.8510\n",
      "training 0.8863848717161291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6673.6704 - mae: 29.1296\n",
      "training 0.8863848717161291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aa1159e7b15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_ACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mfuture_q_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mmaxfuture_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_q_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmaxfuture_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       \u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m     \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \"\"\"\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    593\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    618\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 620\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2744\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2746\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2747\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m         tld.op_callbacks, dataset, iterator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory_actions = []\n",
    "epsilon = 1\n",
    "batch_size = 64\n",
    "episodes = []\n",
    "for i in range(2000):\n",
    "    total_reward = 0\n",
    "    obs = env.reset().reshape(1,-1)\n",
    "    # the action the model takes is the output with the highest value\n",
    "    action = np.argmax(target_model.predict(obs))\n",
    "    done = False\n",
    "    while not done:\n",
    "        lastobs = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        obs = obs.reshape(1,-1)\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(target_model.predict(obs))\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        step = [lastobs,action,reward,obs]\n",
    "        memory_actions.append(step)\n",
    "        \n",
    "        \n",
    "        if len(memory_actions) > 1000:\n",
    "            print(\"training \" +str(epsilon))\n",
    "            # do training once we've sampled enough actions\n",
    "            batch = np.asarray(random.sample(memory_actions,batch_size))\n",
    "            current_states = np.concatenate([i[0] for i in batch])\n",
    "            cur_q_vals = target_model.predict(current_states)\n",
    "            next_states = np.concatenate([i[3] for i in batch])\n",
    "            rewards = np.array([i[2] for i in batch])\n",
    "            actions = to_categorical(np.array([i[1] for i in batch]),num_classes=NUM_ACTIONS)\n",
    "            future_q_vals = target_model.predict(next_states)\n",
    "            maxfuture_q = np.amax(future_q_vals,axis=1)\n",
    "            updates = rewards + 0.999*maxfuture_q\n",
    "            np.putmask(cur_q_vals,actions,updates.astype('float32',casting='same_kind'))\n",
    "            target_model.fit(current_states,cur_q_vals,batch_size=batch_size)\n",
    "\n",
    "    episodes.append(total_reward)\n",
    "    epsilon*=0.99\n",
    "    \n",
    "    plt.plot([i for i in range(len(episodes))],episodes)\n",
    "    if i % 20 == 0:\n",
    "        plt.show()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_q_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = np.asarray([69,69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.putmask(x,act,updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69,  2],\n",
       "       [ 3, 69]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len is not well defined for symbolic Tensors. (activation_4/Identity:0) Please call `x.shape` rather than `len(x)` for shape information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-438900902d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m dqn = DQNAgent(model=model, nb_actions=env.action_space.shape, memory=memory, nb_steps_warmup=10,\n\u001b[0m\u001b[1;32m     40\u001b[0m                target_model_update=1e-2, policy=policy)\n\u001b[1;32m     41\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Validate (important) input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model \"{}\" has more than one output. DQN expects a model that has a single output.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\u001b[0m\u001b[1;32m    753\u001b[0m                     \u001b[0;34m\"Please call `x.shape` rather than `len(x)` for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \"shape information.\".format(self.name))\n",
      "\u001b[0;31mTypeError\u001b[0m: len is not well defined for symbolic Tensors. (activation_4/Identity:0) Please call `x.shape` rather than `len(x)` for shape information."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.shape, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
