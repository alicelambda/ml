{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-859580025325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorforce import Agent, Environment\n",
    "import time\n",
    "# Pre-defined or custom environment\n",
    "environment = Environment.create(\n",
    "    environment='gym', level='Breakout-ram-v0', max_episode_timesteps=500\n",
    ")\n",
    "\n",
    "# Instantiate a Tensorforce agent\n",
    "agent = Agent.create(\n",
    "    agent='tensorforce',\n",
    "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
    "    memory=10000,\n",
    "    update=dict(unit='timesteps', batch_size=64),\n",
    "    optimizer=dict(type='adam', learning_rate=3e-4),\n",
    "    policy=dict(network='auto'),\n",
    "    objective='policy_gradient',\n",
    "    reward_estimation=dict(horizon=20)\n",
    ")\n",
    "\n",
    "# Train for 300 episodes\n",
    "for _ in range(300):\n",
    "\n",
    "    # Initialize episode\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "\n",
    "    while not terminal:\n",
    "        # Episode timestep\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        time.sleep(1)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "\n",
    "agent.close()\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent\n",
    "import numpy as np\n",
    "\n",
    "class Smol(Environment):\n",
    "    \"\"\"This class defines a simple thermostat environment.  It is a room with\n",
    "    a heater, and when the heater is on, the room temperature will approach\n",
    "    the max heater temperature (usually 1.0), and when off, the room will\n",
    "    decay to a temperature of 0.0.  The exponential constant that determines\n",
    "    how fast it approaches these temperatures over timesteps is tau.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        ## Some initializations.  Will eventually parameterize this in the constructor.\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=(1,))\n",
    "\n",
    "\n",
    "    def actions(self):\n",
    "\n",
    "        return dict(type='int', num_values=10)\n",
    "\n",
    "\n",
    "    # Optional, should only be defined if environment has a natural maximum\n",
    "    # episode length\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "\n",
    "    # Optional\n",
    "    def close(self):\n",
    "        super().close()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        return self.current_temp\n",
    "\n",
    "\n",
    "    def response(self, action):\n",
    "        \"\"\"Respond to an action.  When the action is 1, the temperature\n",
    "        exponentially decays approaches 1.0.  When the action is 0,\n",
    "        the current temperature decays towards 0.0.\n",
    "        \"\"\"\n",
    "        return action + (self.current_temp - action) * math.exp(-1.0 / self.tau)\n",
    "\n",
    "\n",
    "    def reward_compute(self):\n",
    "        \"\"\" The reward here is 0 if the current temp is between 0.4 and 0.6,\n",
    "        else it is distance the temp is away from the 0.4 or 0.6 boundary.\n",
    "        \n",
    "        Return the value within the numpy array, not the numpy array.\n",
    "        \"\"\"\n",
    "        delta = abs(self.current_temp - 0.5)\n",
    "        if delta < 0.1:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return -delta[0] + 0.1\n",
    "\n",
    "\n",
    "    def execute(self, actions):\n",
    "        ## Check the action is either 0 or 1 -- heater on or off.\n",
    "        assert actions == 0 or actions == 1\n",
    "\n",
    "        ## Increment timestamp\n",
    "        self.timestep += 1\n",
    "        \n",
    "        ## Update the current_temp\n",
    "        self.current_temp = self.response(actions)\n",
    "        \n",
    "        ## Compute the reward\n",
    "        reward = self.reward_compute()\n",
    "\n",
    "        ## The only way to go terminal is to exceed max_episode_timestamp.\n",
    "        ## terminal == False means episode is not done\n",
    "        ## terminal == True means it is done.\n",
    "        terminal = False\n",
    "        if self.timestep > self.max_episode_timesteps():\n",
    "            terminal = True\n",
    "        \n",
    "        return self.current_temp, terminal, reward\n",
    "\n",
    "###-----------------------------------------------------------------------------\n",
    "### Create the environment\n",
    "###   - Tell it the environment class\n",
    "###   - Set the max timestamps that can happen per episode\n",
    "environment = environment = Environment.create(\n",
    "    environment=ThermostatEnvironment,\n",
    "    max_episode_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
